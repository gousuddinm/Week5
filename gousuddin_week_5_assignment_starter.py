# -*- coding: utf-8 -*-
"""Gousuddin_Week_5_assignment_starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wsFDStntIikU775aQZl7Q8j9q53SrK8e

### Name : Gousuddin Mohammad

# DS Automation Assignment

Using our prepared churn data from week 2:
- use pycaret to find an ML algorithm that performs best on the data
    - Choose a metric you think is best to use for finding the best model; by default, it is accuracy but it could be AUC, precision, recall, etc. The week 3 FTE has some information on these different metrics.
- save the model to disk
- create a Python script/file/module with a function that takes a pandas dataframe as an input and returns the probability of churn for each row in the dataframe
    - your Python file/function should print out the predictions for new data (new_churn_data.csv)
    - the true values for the new data are [1, 0, 0, 1, 0] if you're interested
- test your Python module and function with the new data, new_churn_data.csv
- write a short summary of the process and results at the end of this notebook
- upload this Jupyter Notebook and Python file to a Github repository, and turn in a link to the repository in the week 5 assignment dropbox

*Optional* challenges:
- return the probability of churn for each new prediction, and the percentile where that prediction is in the distribution of probability predictions from the training dataset (e.g. a high probability of churn like 0.78 might be at the 90th percentile)
- use other autoML packages, such as TPOT, H2O, MLBox, etc, and compare performance and features with pycaret
- create a class in your Python module to hold the functions that you created
- accept user input to specify a file using a tool such as Python's `input()` function, the `click` package for command-line arguments, or a GUI
- Use the unmodified churn data (new_unmodified_churn_data.csv) in your Python script. This will require adding the same preprocessing steps from week 2 since this data is like the original unmodified dataset from week 1.
"""

!pip install pycaret

import pandas as pd
import numpy as np
from pycaret.classification import *

data = pd.read_csv('churn_data.csv', index_col='customerID')
data.fillna(data['TotalCharges'].median(), inplace=True)
yn_dict = {'Yes': 1, 'No': 0}
data['PhoneService'] = data['PhoneService'].replace(yn_dict)
data['PaymentMethod'] = data['PaymentMethod'].replace({'Electronic check': 3, 'Mailed check': 2, 'Bank transfer (automatic)': 1, 'Credit card (automatic)': 0})
data['Contract'] = data['Contract'].replace({'Month-to-month': 0, 'One year': 1, 'Two year': 2})
data['Churn'] = data['Churn'].replace(yn_dict)
data.loc[data['tenure'] == 0, 'tenure'] = np.nan
data['tenure'].fillna(data['tenure'].median(), inplace=True)
data['charge_per_tenure'] = data['TotalCharges'] / data['tenure']

# Setup PyCaret
clf = setup(data=data, target='Churn', session_id=123, verbose=False)


# Compare models
best_model = compare_models(sort='AUC')

# Save the best model
save_model(best_model, 'best_model_churn')

new_data = pd.read_csv('new_churn_data.csv')

new_data.head()

import pandas as pd
from pycaret.classification import load_model, predict_model

def preprocess_data(data):
    yn_dict = {'Yes': 1, 'No': 0}
    data['PhoneService'] = data['PhoneService'].replace(yn_dict)
    data['PaymentMethod'] = data['PaymentMethod'].replace({'Electronic check': 3, 'Mailed check': 2, 'Bank transfer (automatic)': 1, 'Credit card (automatic)': 0})
    data['Contract'] = data['Contract'].replace({'Month-to-month': 0, 'One year': 1, 'Two year': 2})
    return data

def predict_churn(dataframe):
    model = load_model('best_model_churn')
    predictions = predict_model(model, data=dataframe)
    return predictions[['Label', 'Score']] if 'Label' in predictions.columns else predictions

if __name__ == "__main__":
    # Load new data
    new_data = pd.read_csv('new_churn_data.csv')
    # Preprocess the new data
    new_data_processed = preprocess_data(new_data)
    # Predict
    predictions = predict_churn(new_data_processed)
    print(predictions)

"""# Summary

Write a short summary of the process and results here.

### Overview of the Process
1. **Initial Data Preparation**: The churn data underwent an initial preprocessing phase. This involved addressing missing values, transforming categorical data into numerical formats, and generating new features.

2. **Choosing a Model with PyCaret**: Various machine learning models were assessed using PyCaret to determine the most effective one. Evaluation criteria included metrics like Accuracy, AUC, Recall, Precision, F1 Score, Kappa, and MCC.

3. **Top Performing Model**: The Gradient Boosting Classifier (GBC) was identified as the superior model, demonstrating an Accuracy of 0.7929, AUC of 0.8376, and F1 Score of 0.5623.

4. **Saving the Model**: The optimal model was preserved for future predictive applications.

5. **Applying the Model to Fresh Data**: This saved model was applied to new data (`new_churn_data.csv`), which was preprocessed in a manner consistent with the training data.

### Performance on the New Dataset
Predictions were made on the new dataset, offering insights into both the likelihood of churn (prediction labels) and the probability of each outcome (prediction scores). The outcomes were as follows:

| CustomerID | Tenure | PhoneService | Contract | PaymentMethod | MonthlyCharges | TotalCharges | ChargePerTenure | PredictionLabel | PredictionScore |
|------------|--------|--------------|----------|---------------|----------------|--------------|-----------------|-----------------|-----------------|
| 9305-CKSKC | 22     | 1            | 0        | 2             | 97.40          | 811.70       | 36.90           | 0               | 0.5953          |
| 1452-KNGVK | 8      | 0            | 1        | 1             | 77.30          | 1701.95      | 212.74          | 0               | 0.9099          |
| 6723-OKKJM | 28     | 1            | 0        | 0             | 28.25          | 250.90       | 8.96            | 0               | 0.8978          |
| 7832-POPKP | 62     | 1            | 0        | 2             | 101.70         | 3106.56      | 50.11           | 1               | 0.5153          |
| 6348-TACGU | 10     | 0            | 2        | 1             | 51.15          | 3440.97      | 344.10          | 0               | 0.8580          |

### Analysis
- The model's predictions indicated that 4 of the 5 customers in the new dataset are not likely to churn, supported by relatively high confidence scores.
- A single customer (CustomerID: 7832-POPKP) was forecasted to churn, with a prediction score of 0.5153 indicating a borderline chance of churning.

### Final Thoughts
- The Gradient Boosting Classifier emerged as the most proficient model for this dataset, based on the selected evaluation metrics.
- Insights gleaned from the model's predictions on the new data are crucial for understanding customer tendencies and could play a vital role in developing strategies to diminish churn.
"""